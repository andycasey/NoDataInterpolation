% Copyright 2022 David W. Hogg (NYU) and Andy Casey (Monash)

% To-Do items
% -----------
% - Audit and fix the sign applied to $\Delta x_i$ everywhere. Wrong relative to code?
% - Write about the two input-data cases.
% - See bug list in code for other issues with the text--code relationship (noise model; line-formation model).
% - Copy in a figure macro from Hogg & Villar and use it.
% - Deal with all HOGG, ANDY, ARC, XXX, YYY, CITE, etc.

\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

% page and document setup
\addtolength{\topmargin}{-0.35in}
\addtolength{\textheight}{0.6in}
\setlength{\parindent}{3.5ex}
\renewcommand{\paragraph}[1]{\medskip\par\noindent\textbf{#1}~---}

% figure setup
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{shadows}
\definecolor{captiongray}{HTML}{555555}
\mdfsetup{%
  innertopmargin=2ex,
  innerbottommargin=1.8ex,
  linecolor=captiongray,
  linewidth=0.5pt,
  roundcorner=1pt,
  shadow=false,
}
\newlength{\figurewidth}
\setlength{\figurewidth}{0.75\textwidth}

% text macros
\shorttitle{don't interpolate your data!}
\shortauthors{hogg \& casey}
\newcommand{\sectionname}{Section}

% math macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\mps}{\unit{m\,s^{-1}}}
\newcommand{\kmps}{\unit{km\,s^{-1}}}

\sloppy\sloppypar\raggedbottom\frenchspacing
\begin{document}

\title{\Huge Don't interpolate your data!}

\author[0000-0003-2866-9403]{David W. Hogg}
\affiliation{Center for Cosmology and Particle Physics, Department of Physics, New York University}
\affiliation{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
\affiliation{Flatiron Institute, a division of the Simons Foundation}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\affiliation{School of Physics \& Astronomy, Monash University}
\affiliation{Faculty of Information Technology, Monash University}

\begin{abstract}\noindent
When there are many observations of an astronomical source---many images with different dithers, or many spectra taken at different barycentric velocities---it is often tempting to shift and stack the data, to (for example) make a high signal-to-noise average image or mean spectrum.
Bound-saturating measurements are made not by manipulating data, but instead by manipulating a likelihood function, where the data are treated as fixed, and model parameters are modified to fit the data.
Traditional shifting and stacking of data can be converted into a model-fitting procedure, such that no data are ever harmed, and yet the output is the shift-adjusted mean.
The key component of this conversion is a spectral model that is completely flexible but also a continuous function of wavelength (or position in the case of imaging) that can represent any signal being measured by the device after any reasonable translation.
The benefits of a modeling approach are myriad:
The sacred data never are modified.
Noise maps, data gaps, and bad-data masks don't require interpolation.
The output can take the form of a spectrum evaluated on a pixel grid, as is traditional.
The noise in the output spectrum becomes uncorrelated across neighboring pixels.
The only cost is a small increase in computational complexity.
We demonstrate all of these things with a small data example and we provide open-source sample code for re-use.
\end{abstract}

\keywords{Foo --- Bar --- HOGG}

\section*{}\clearpage
\section{Introduction}\label{sec:intro}

It would be wrong to say that the practice of astronomy is, basically, \emph{staring at the sky}.
That said, staring at the sky is a big part of what we do.
And the way we do it, usually, is this:
We take many individual images of our target with exposure times that are long enough to make read noise irrelevant (if possible) while at the same time short enough to avoid saturation (of what we care about).
We then shift and average (maybe median?) those images to get the highest possible signal-to-noise on our target.
This, fundamentally, is the technology underlying the \textsl{Hubble Deep Field} (\citealt{hdf}),
the \textsl{APOGEE} spectroscopic survey (\citealt{apogee}),
and the future Rubin Observatory \textsl{LSST} (\citealt{lsst}), among countless other projects.

Is it astronomical blasphemy to say that this practice is \emph{wrong?}
It is wrong because minimum-variance unbiased estimators require care in their construction; they are (often) maximum-likelihood estimators (\citealt{something}).
The deep, high signal-to-noise image or spectrum we seek is a statistical estimate of the image or spectrum, given the noisy data of the individual exposures.
Noisy data are combined to make a measurement by building and optimizing a likelihood, not by shifting and stacking the input data.
Of course we don't always think of an image or a spectrum of the sky as the result of applying an \emph{estimator},
but it is.
It is just a simultaneous estimate of a lot of different components (a lot of different pixels; all the pixels in the combined image or spectrum).

We are in a period in astrophysics in which exceedingly precise measurements are expected, in both imaging and spectroscopy projects.
In spectroscopy, in particular, high signal-to-noise spectral measurements are expected to obtain surface-abundance measurements with precisions a few percent, and radial-velocity measurements with precisions of up to one ten-thousandth of a \emph{pixel} (in terms of the Doppler shift).
Spectral combination methods that distort absorption (or emission) lines will translate into biases, errors, or increased variance in these measurements.
Spectral combination methods that correlate the noise in the outputs will translate into increased variance, and necessitate more sophisticated downstream measurement techniques (which don't really exist at present).

HOGG: Of course if your GOAL IS RV you should DO SOMETHING DIFFERENT.

One of the great things about the standard practice---the simple process of shifting and stacking the data---is that this procedure makes almost no assumptions about what the output, combined image or spectrum will look like.
It isn't really a ``model'' for the data; it is more like a ``summary'' of the data.
When we switch to a statistical-inference framework, we have to choose some form or basis for expressing the combined image.
In what follows we use a maximally parameterized model, in the sense that we choose an expression for the functions or function space that has as many parameters as the output image has pixels.
That means that the approach we take does not get its power from the choice of parameterized functions for the output:
Indeed it has just as much freedom as the shifting-and-stacking standard practice.
The forward-modeling we recommend gets its power from the fact that it generates the data directly and without manipulation.

Things to emphasize here and below:
\begin{itemize}
\item Maybe move this to the method section? People have a lot of hangups about the word ``model''. We have to use another word, or be very very careful how we use this word. For instance, some people I talked to about this thought I meant a physical model of the spectrum starting with atomic physics!
\item All this is even more important in ill-sampled contexts than well-sampled contexts. See, for example, \textsl{APOGEE} raw data!
\item We should choose a rigid terminology and stick to it here. HOGG is currently liking ``combined spectrum'' for the model expectation.
\item This problem is a sub-problem of \textsl{wobble} \cite{wobble}.
\item The problem we solve here is the easy case. In the hard case, there is a slit, and the object is extended, and different observations are at different slit angles, different slit positions, and with different atmospheric seeing. That problem is a hard problem! But it probably solves the same way. Say a bit here and more in the discussion.
\end{itemize}

Most important: Say here why you should never want to do this at all!

\section{Concepts and assumptions}\label{sec:assumptions}

What we do here can apply to almost any astronomical signals, but we are going to specialize to astronomical spectra for specificity.
We are going to make many other assumptions too, all of which are discussed in this \sectionname.

\paragraph{Multi-epoch spectra with shifts}
We will assume that there are $n$ observations (epochs) of the same star (or maybe of different stars or quasars, say, that are assumed to be more-or-less identical, intrinsically, but let's think of a single star for now).
Each of these observations $i$ (with $1\leq i\leq n$) produces a one-dimensional observed (noisy) image $y_i$.
The $y_i$ can be thought of as $m_i$-vectors in what follows, where $m_i$ is the number of pixels in the spectrum.
The different observations are made at different relative velocities (star minus spectrograph) such that the different images are shifted by a different shift $\Delta x_i$ relative to the detector or extracted wavelength grid.

\paragraph{Shift operators}
Because the shifts might be non-linear, the shifts $\Delta x_i$ won't be pure numbers but something more like shift operators.
That is, if there were a true spectrum $\tilde{y}$, each individual noisy image $y_i$ would be related to that true spectrum by a shift operator acting on that true spectrum, plus noise.
We will begin by assuming that (from some external information) we know these shift operators very accurately.
At the end we may (do we?) return to the question of how those shift operators are determined.

\paragraph{Wavelength calibration}
We will assume that, for every $m_i$-pixel image $y_i$ we also have a $m_i$-vector (or list) $x_i$ of pixel positions, such that each element of $x_i$ gives an accurate and precise value for the wavelength (or position in the spectrograph, in some settings) for each pixel of the image $y_i$.
That is, the device is well calibrated, and we know all the housekeeping data we need for image $y_i$.

\paragraph{Noise model}
Each observation is noisy.
We will assume that the noise has known properties, and especially that it is additive, (nearly) zero-mean and (nearly) Gaussian in form, with known variance.
Under these assumptions, each spectrum $y_i$ has an associated variance tensor $C_i$.
If $y_i$ is an $m_i$-vector, then $C_i$ is a nonnegative-definite $m_i\times m_i$ 2-tensor.
We don't need to assume that the different pixels of the spectrum received independent noise, but we do assume that each epoch $i$ has an independent noise draw.

\paragraph{Bad-pixel masks}
Occasionally (or frequently) some of the pixels of a spectrum $y_i$ might be bad---affected by cosmic rays or electronics issues---such that there is a bad-pixel mask $b_i$ associated with each spectrum $y_i$.
This mask has value 1 in the locations in all good pixels, and 0 in the locations of all bad pixels.

\paragraph{Data gaps}
There is no assumption here that all the $y_i$ have the same number of pixels $m_i$, nor that the pixel grid is in any sense uniform or complete.
That is, there can be gaps and spaces in the wavelength coverage of the spectra, as there are at chip gaps and where the detector has bad columns.

\paragraph{Pixel-convolved line-spread function}
The spectrograph has something like a point-spread function or line-spread function, which, in the case of spectrosccopy, sets the shape of an unresolved spectral line.
In general it depends on the slit or fiber width, the spectrograph resolution and optics, instrument focus, and the properties of the detector.
In what follows we will not build any model of any of this.
Instead we will assume that there is a relatively constant pixel-convolved line-spread function (PCLSF), constant both in time, and constant with respect to the shifts $\Delta x_i$ we see in our data set.
We consider only the PCLSF, because then the sampling by the detector pixels does not require any additional convolution.
That is, when the PCLSF is used to model a spectrum, the projection onto the pixels is just a sampling of the function at the pixel centers, and does not involve any convolution over the face of the pixel.
All that said, we won't explicitly model or use the PCLSF in what follows; we will just assume that our continuous spectral model is a model for the pixel-convolved, finite-resolution spectrum; we won't be doing any convolutions in comparing models and data.

HOGG: Something about the LSF being different in different exposures!

\paragraph{Continuous spectral model}
In what follows we are going to avoid interpolating the \emph{data} by having a spectral \emph{model} that can be arbitrarily and losslessly interpolated.
That means that the spectral model must be a representation of a continuous one-dimensional function.
If we were working on two-dimensional images, the model would have to be a representation of a continous two-dimensional function.

\paragraph{Linear basis or mixture models}
The simplest kind of model for a continuous function is a sum or mixture or linear combination of continuous basis functions.
These could be sines and cosines for a Fourier series model.
They could be spline (or sinc or Lanczos) cardinal basis functions.
They could be wavelets.
The point is that if we use a linear basis, and assume that noises are Gaussian, we will get closed-form expressions for the data combination.
Thus we will use a linear basis.

\paragraph{Band limit}
We say that a signal is \emph{band-limited} if it has no frequency content above some highest possible cutoff frequency.
There is a weird sense in which data from a spectrograph both is, and is not, band limited.
It is band limited in that the spectrograph is finite in resolution, such that no real spectroscopic signal can show features narrower than the PCLSF.
It is not band-limited in that the photon noise in the device is independent (or close to independent) from pixel to pixel, such that the noise contribution to each observation $y_i$ has support at all spatial frequencies.

One interesting question in what follows is whether the forward model we build for the data should itself be restricted to the band limit defined by the spectrograph resolution, or whether it should permit higher frequencies.
It might surprise the reader to learn that we prefer to let the model capture higher frequencies, even when we think they shouldn't be there for any reasons other than noise, given the hardware.
More on this below.

\paragraph{Critical sampling}
Because of considerations related to the Nyquist frequency on a uniform grid, it is valuable for a spectrograph to have a pixel spacing on the device that is smaller than the width of the PCLSF.
Specifically, it is valuable for there to be two or three pixels across the full-width at half-maximum (FWHM) of the PCLSF.
This keeps the band-limited part of the spectroscopic signal well-sampled in the data.
There are spectrograph that don't have pixel scales this fine \cite{apogeehardware}; in these cases what we are proposing here is even more important than it is in the well-sampled case.

\paragraph{Non-uniform fast Fourier transform}
In the special case that the spectral model is a Fourier series, there are very useful and fast tools for interpolation, including especially the non-uniform fast Fourier transform (for example, \cite{finufft}).

\paragraph{Time variability}
When we average data (or build a static model of multi-epoch data), we are implicitly assuming that there is no intrinsic variability to the source being observed.
That is, we are assuming that the changes from observation to observation are solely from either the shifts $\Delta x_i$ or from the noise.
We interpolate the model to account for the shifts, and we combine data to beat down the noise.
There are situations in which there is substantial time variability and these approaches are still valid, and there are modified approaches.
We will discuss these changes and generalizations at the end.

\paragraph{Bias, sky, tellurics, flat-field, normalization}
In what follows, we will assume that the input data are well calibrated in various ways.
In particular, we will assume that the images or spectra have been bias-corrected, sky-subtracted, flat-fielded, and tellurics-corrected sufficiently well that the different observations $y_i$ can be seen as observations of the same underlying or intrinsic signal.
Alternatively, the input data might be continuum-normalized or pseudo-continuum-normalized.
Again, the assumption is that the different calibrated or processed observations are of the same underlying signal.

\section{Method}\label{sec:method}

The problem set-up is that there are $N$ spectral measurements $y_i$ (with $1\leq i\leq N$), each of which is a $M_i$-pixel list of intensities (fluxes maybe).
Different observations $i$ might have different numbers $M_i$ of pixels.
The $M_i$ pixels are associated with an $M_i$-length list of positions (wavelengths or log-wavelengths) $x_i$.
Each measurement or observation $i$ is shifted by a shift operator $\Delta x_i$, which can be thought of as a length-$M_i$ list of pixel offsets or shifts, such that the ``rest frame'' or ``unshifted'' positions corresponding to the length-$M_i$ list of data $y_i$ is the list of differences $x_i - \Delta x_i$.
The goal is to get a combined spectrum that represents the mean of these spectra, in the common rest frame.

The spectral expectation model $f(x;\theta)$, which will be a representation for our combined spectrum, is a function of $x$ controlled by a list $\theta$ of $P$ linear parameters $a_j$ (with $1\leq j\leq P$).
This function will be very flexible; possibly even an interpolation of control points or a Fourier series; the number $P$ will be roughly the number of pixels we want in our output combined spectrum.
The statistical model underlying can be expressed qualitatively as
\begin{align}
    y_i &= f(x_i + \Delta x_i;\theta) + \mbox{noise} \\
    f(x;\theta) &= \sum_{j=1}^P a_j\,g_j(x) ~,
\end{align}
where we are being a bit loose with terminology (since $x_i$ is a set of positions, not just one position), the noise is additive (but not yet specified), and each of the $P$ functions $g_j(x)$ is a basis function of the representation.

The noise model is additive (as noted above) and we will assume that the noise contributing to observation $y_i$ is zero-mean and Gaussian, with a known $M_i\times M_i$ variance tensor $C_i$ (or its inverse $C_i^{-1}$).
If the raw pixel measurements are independent, which they often are (and are often assumed to be), then the $C_i$ (or $C_i^{-1}$) matrices will be diagonal matrices with the individual pixel variances (or inverse variances) down the diagonals.
In many cases you don't know (or aren't provided with) the individual pixel variances, and you are just co-adding data with uniform weights.
That choice, for our purposes, is equivalent to setting every inverse variance matrix $C_i^{-1}$ to the $M_i\times M_i$ identity matrix.
That's permitted!
It is no worse a choice than it was (in the past) when you averaged your input data without thinking about noise variances or weights.
Another sensible default choice would be to set each inverse matrix $C_i^{-1}$ to the identity times the exposure time, such that the eventual math we do will weight the data by the exposure times.
That is, you do not need to know the noise variances accurately (or even at all) to execute the forthcoming procedure; but if you do know them, it is good to use them.

If you additionally have a bad-pixel mask $b_i$ associated with each observation $y_i$, the bad-pixel mask can be used simply to zero out the corresponding bad rows and columns of the inverse covariance matrix $C_i^{-1}$.
Importantly, the bad data get zeros in the \emph{inverse} covariance matrix, not the covariance matrix!

Because we are often dealing with a spectrograph with a (fairly) well defined spectral resolution ($\delta\lambda/\lambda$ roughly constant), and because we care a lot about Doppler shifts (which are uniform in log wavelength), it makes sense to think of the pixel positions $x_i$ and shifts $\Delta x_i$ as being in log-wavelength (or ln-wavelength) units.
Indeed, the \textsl{Sloan Digital Sky Survey} family of spectrographs extract one-dimensional spectra on a uniform-in-log-wavelength basis.
Once that choice is made, there are still many choices for the linear basis,
which include canonical functions from interpolation bases, wavelets, and Fourier series.
In what follows, somewhat arbitrarily, we will use a Fourier basis, which can be expressed as follows:
\begin{align}
    g_j(x) & = \left\{\begin{array}{cl}\displaystyle\cos\left(\frac{\pi\,[j-1]}{L}\,x\right) & \mbox{for $j$ odd} \\[3ex]
                                       \displaystyle\sin\left(\frac{\pi\,j}{L}\,x\right) & \mbox{for $j$ even}\end{array}\right. ~,
\end{align}
where $L$ is a (long) length-scale in the $x$ space (the log-wavelength space), and $1\leq j\leq P$.

Now for the method:
We stack all of the observations $y_i$ into one huge $M$-pixel list $Y$, where $M=\sum_{i=1}^N M_i$.
We make a $M\times M$ block-diagonal matrix $C^{-1}$ from all the individual inverse variance matrices $C_i^{-1}$.
We make a $M\times P$ design matrix $X$ which is the evaluation of all the $P$ basis functions at all the pixel locations in all of the $x_i$.
Once we have these things, the best-fit parameters $\hat\theta$ are
\begin{align}
    \hat\theta &= (X^\top\,C^{-1}\,X)^{-1}\,X^\top\,C^{-1}\,Y ~,
\end{align}
which is the maximum-likelihood or minimum-chi-squared solution for a linear weighted least squares problem or a linear model with Gaussian noise (see, for example, \cite{hoggfitting}).
And now if we choose a final (uniform, say) output pixel grid $x_\star$ in the rest-frame (unshifted) wavelength space, the combined (output, maybe?) spectrum is just
\begin{align}
    y_\star &= X_\star\,\hat\theta ~,
\end{align}
where $X_\star$ is the $M_\star\times P$ evaluation of all of the $P$ basis functions at the locations of the $M_\star$ pixels in the output-spectrum pixel grid $x_\star$.
Putting these things together, the output spectrum $y_\star$ can be thought of as just a kind of linear shifting-and-averaging operation on the input spectra $y_i$ (which have been packed into $Y$) according to
\begin{align}
    y_\star &= A_\star\,Y \\
    A_\star &\equiv X_\star\,(X^\top\,C^{-1}\,X)^{-1}\,X^\top\,C^{-1} ~.
\end{align}
That is, the method we are delivering is a linear combination of the input data, just like traditional interpolate-and-average.
But our method never involves, not even implicitly, shifting or interpolating the input data.

The uncertainty on the combined spectrum $y_\star$ can be estimated in a few ways.
The information-theoretic answer is that the covariance matrix $C_\star$ delivering the uncertainties on $y_\star$ is
\begin{align}
    C_\star &= [X_\star\,(X^\top\,C^{-1}\,X)^{-1}\,X_\star^\top]^{-1} ~.
\end{align}
The naive uncertainty variances are the diagonals of this matrix (and not the inverses of the diagonals of its inverse).
But, \emph{very importantly}, this is only a correct usage when the input $C_i$ matrices really represented the true uncertainties going in.
If you have any concerns about that, then it is way safer to get your uncertainty estimates from bootstrap or jackknife \cite{hoggflexible}.

In the end, most users want the combined spectrum $y_\star$ (plus the pixel positions $x_\star$ and uncertainty information).
Some users will additionally want (or want to reconstruct) the parameters $\hat\theta$ and the basis functions $g_j(x)$.
For these reasons it often makes sense to set $M_\star\geq P$ such that the delivered $y_\star$ fully determines the parameters.
But that's a detailed question about your users and your goals.

\paragraph{Implementation notes}
Don't actually construct the very sparse $C_i$ matrices, just do pixel-wise multiplies.
Don't use \texttt{inv()}, use \texttt{solve()} or \texttt{lstsq()}.
Check your condition numbers before you go.
If $X$ is either very sparse (as it is in a b-spline basis) or Fourier (as it is in a sine-and-cosine basis) then there are very fast ways to do the linear algebra; research those.
If you use the nufft, order your frequencies in the way the software wants, and deal with the complex numbers.
Specific choice of $L$ and $P$ and $M_\star$ in the Fourier case.

\section{Experiments and results}\label{sec:results}

\paragraph{Fake data}
We demonstrate the effectiveness of the spectral combination method on artificial data.
The spectral data were generated from a mean spectral expectation that is a continuum plus a randomly generated (not at all physical) line list, where each line equivalent width was set by a random draw from a power law.
Each spectral epoch was given a velocity (Doppler shift) by putting the epochs equally spaced on a sinusoidal variation with a semi-amplitude of $30\,\kmps$ (to emulate the motion of the spectrograph with respect to the Solar System barycenter).
The spectral expectation model is built on the assumption of a normalized (unit) continuum and unresolved spectral lines with a Gaussian line-spread function (LSF) with one-sigma width $1/R$, where $R=135\,000$ is the spectrograph resolution).

The sampled data at each epoch were made by simply evaluating the spectral expectation model (shifted by the Doppler shift) at the pixel centers, such that the $R=135\,000$ LSF is effectively the pixel-convolved LSF.
After sampling the mean spectral expectation model onto an observational pixel grid, Gaussian noise is added with a variance that grows linearly with expected flux, such that the signal-to-noise per pixel in the continuum has a definite known value in each epoch spectrum.
At a rate of 0.01 (one percent), sets of three adjacent pixels were randomly and independently marked as ``bad'' and offset by a large positive offset.

According to these rules we made two input data sets.
One is poorly sampled, with $M=171$ spectral pixels in the input data separated by $2/R$ in $x$ (natural logarithm of wavelength), and signal-to-noise per pixel of 18 in the continuum.
The other is well sampled, with $M=340$ spectral pixels in the input data separated by $1/R$ in $x$, and signal-to-noise per pixel of 12 in the continuum.
In each case there are $N=8$ epochs.
The two complete data sets are shown in \figurename~\ref{fig:data1} and \figurename~\ref{fig:data2}.
\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=1.3\figurewidth]{notebooks/data1.pdf}\\
    \includegraphics[width=\figurewidth]{notebooks/datazoom.pdf}
    \end{center}
    \caption{The top 8 panels show the $N=8$ epochs of the poorly sampled input data, with $N=8$, $M=171$, and signal-to-noise per pixel of 18 in the continuum. The bad pixels are indicated with grey bars. The bottom 2 panels show a zoom in on one of the epochs, along with the true spectrum used to generate the data. The true spectrum is shown at zero Doppler shift, while each epoch spectrum is at a finite Doppler shift $\Delta x_i$ relative to the spectrograph pixel grid.\label{fig:data1}}
    \end{mdframed}
\end{figure}
\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=1.3\figurewidth]{notebooks/data2.pdf}
    \end{center}
    \caption{Similar to the top panels of \figurename~\ref{fig:data1} except showing the well sampled input data with $N=8$, $M=340$, and signal-to-noise per pixel of 12 in the continuum.\label{fig:data2}}
    \end{mdframed}
\end{figure}

\paragraph{Forward Model (tm)}
As our main experiment or result, we apply the Forward Model (tm) as described above in \sectionname~\ref{sec:method} to the input data shown in \figurename~\ref{fig:data1} and \figurename~\ref{fig:data2}.
In each case, the output pixel grid $x_\star$ is chosen to be well sampled, with a pixel spacing of $1/R$ in the (natural) logarithm of wavelength.
In order to give the model maximum flexibility, the number $P$ of Fourier modes in the spectral model is set to be equal to the number of pixels $M_\star$ in the pixel representation.
The log-wavelength scale $L$ of the Fourier basis was set to the pixel spacing times the number of pixels.
We didn't employ any data weighting; we treated every epoch as identical in terms of inverse variance.

\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=\figurewidth]{notebooks/forward1.pdf}\\
    \includegraphics[width=\figurewidth]{notebooks/forward2.pdf}
    \end{center}
    \caption{The top two panels show the result of running the Forward Model (tm) on the poorly sampled input data, plus the residual away from the truth. The bottom two panels show the same but for the well sampled input data. In each panel, the red line shows the true model used to generate the data.\label{fig:forward}}
    \end{mdframed}
\end{figure}
The combined spectrum resulting from the Forward Model (tm) described in \sectionname~\ref{sec:method} is shown in \figurename~\ref{fig:forward}.
It is compared to the true spectral expectation model that was used to generate the data.
Also shown in \figurename~\ref{fig:forward} is the residuals, Forward Model (tm) minus truth.
The residuals look stationary, approximately Gaussian, and uncorrelated (more on this below).

\paragraph{Standard Practice (tm)}
In standard practice, the individual spectra $y_i$ are interpolated to rest-frame spectra $y'_i$ using an interpolator.
In detail, the data are shifted and also resampled onto the output pixel grid $x_\star$.
The interpolator is a choice; cubic spline, Lanczos, and sinc interpolations are common, since they have good properties with respect to the spectrograph band limit.
Here we choose a cubic spline for convenience, but there are many other choices, including linear, sinc, and Lanczos interpolators, among many others.

The interpolated spectra $y'_i$ are, by construction, all on the same rest-frame wavelength grid $x_\star$ so they can be averaged, with a straight average, a median, an inverse-variance-weighted mean, or any more sophisticated algorithm.
Here we choose an inverse-variance-weighted mean (HOGG: NO WE DON'T), but zeroing out the inverse variance at the locations of the bad pixels.

This censorship of the bad pixels requires some attention:
Naive interpolation of an integer (binary actually) bad-pixel mask $b_i$ will not deliver an integer mask $b'_i$.
The conservative move (which we adopt) is to naively interpolate $b_i$ (also by the cubic spline) and then zero out any pixels that are significantly below unity.
In general this substantially grows the binary mask, and reduces the amount of data available for combination.

\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=\figurewidth]{notebooks/datazoom_interpolated.pdf}
    \end{center}
    \caption{As part of Standard Practice (tm), it is necessary to interpolate the individual epoch spectra before averaging them. This shows one epoch of the poorly sampled data, interpolated to the output $x_\star$ pixel grid in the rest frame. The interpolated spectrum is compared to the true spectral expectation employed to generate it. Note the ringing induced by the interpolation. Also shown is the censored data, near an interpolated bad pixel and at the edge of the domain. Compare to the bottom panels of \figurename~\ref{fig:data1}.\label{fig:interpolated}}
    \end{mdframed}
\end{figure}
An example of a poorly-sampled spectrum interpolated to the rest frame is shown in \figurename~\ref{fig:interpolated}.
The interpolation introduces substantial ringing artifacts in the spectrum (compare to \figurename~\ref{fig:data1}).
It also grows the bad-pixel mask around the bad pixel.

\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=\figurewidth]{notebooks/standard1.pdf}\\
    \includegraphics[width=\figurewidth]{notebooks/standard2.pdf}
    \end{center}
    \caption{Same as \figurename~\ref{fig:forward} but for the output of Standard Practice (tm). Note that in the poorly sampled case, the residuals appear to be spatially correlated, and the residuals appear to be larger near strong spectral features; more about these issues in the text and in \figurename~\ref{fig:noise}.\label{fig:standard}}
    \end{mdframed}
\end{figure}
We show the final result of Standard Practice (tm)---interpolating the input data and then averaging the interpolations---for the two data sets, in \figurename~\ref{fig:standard}.
We also show the comparison with the true spectral expectation model that was employed to make the fake data.
The residuals look correlated (there is some ringing), and the residuals are larger near strong spectral features, especially in the case of the poorly sampled input data.

\paragraph{Noise and noise covariances}
HOGG do repeated trials to empirically measure the noise covariances. How exactly did we do that?

In \figurename~\ref{fig:noise} we show the empirical covariances for the poorly sampled and well sampled data sets.
In each case, Standard Practice (tm) has a lower noise variance at zero lag, but shows spatially correlated noise over many pixels.
The Forward Model (tm), in contrast, shows no pixel-to-pixel noise covariances, even when the input data are ill sampled.
\begin{figure}[t!]
    \begin{mdframed}\begin{center}
    \includegraphics[width=\figurewidth]{notebooks/noise.pdf}
    \end{center}
    \caption{The empirical noise variance in the output (combined) spectra, and pixel-to-pixel covariances, as a function of pixel lag (pixel offset). The variances were estimated by performing the experiments repeated 64 times, with the same spectral expectations and sampling but unique noise draws. The Forward Model (tm) has slightly larger variance per pixel at zero lag, but vanishing pixel-to-pixel correlations. In contrast, the Standard Practice (tm) leads to correlated pixel values, with correlations extending out to many pixels.\label{fig:noise}}
    \end{mdframed}
\end{figure}

HOGG: Note that if we choose $P<M$ we start to see noise correlations in the Forward Model (tm) too.
That's one of a few reasons we like $P=M$.

\section{Discussion}\label{sec:discussion}

Give some summary of the above.

What happens if you don't know your shifts?

What happens if you have multiple shifting signals?
\texttt{wobble} \cite{wobble} is a solution to one version of that problem.

What happens if your data aren't continuum normalized properly?

We assumed no sky subtraction necessary. What gives?

How do you modify to deal with LSF variations (like we have in \textsl{SDSS-V APOGEE}?
What if you don't know the LSF differences?
Difference between modeling LSF changes and modeling LSFs. The former is good, the latter is bad.

Discuss time-variable inputs, and how you would modify to deal with that.
One way involves modeling the time variability with some process.
Another way involves ignoring it and just getting the average anyway!

What about non-Gaussian noise?

What about data from different instruments with different LSFs / PSFs? What about data with different pixel scales? What about data with unknown wavelength calibration?

Code is available at HOGG THIS URL.

\paragraph{Software}
\texttt{numpy} \cite{numpy} ---
\texttt{finufft} \cite{finufft} ---
\texttt{matplotlib} \cite{matplotlib}.

\paragraph{Acknowledgements}
It is a pleasure to thank
Matt Daunt (NYU),
Dustin Lang (Perimeter),
Adrian Price-Whelan (Flatiron),
and the Astronomical Data Group at the Flatiron Institute,
for valuable discussions.

\end{document}
